{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib"
      ],
      "metadata": {
        "id": "pyxsOxhVcZ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip uninstall matplotlib \n",
        "!sudo pip install matplotlib==1.5.1"
      ],
      "metadata": {
        "id": "UvZzNejEa8hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U protobuf\n",
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "id": "Fdkfcl-k7TCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tb-nightly"
      ],
      "metadata": {
        "id": "N3yd5iENbzbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Based KG"
      ],
      "metadata": {
        "id": "0UpOMw6da5Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total Network"
      ],
      "metadata": {
        "id": "_bWXTDOIbAmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 50"
      ],
      "metadata": {
        "id": "60aHDsJZbc5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "#import tensorflow as tf\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "tf.reset_default_graph()\n",
        "#!pip install matplotlib==3.2\n",
        "#tf.compat.v1.disable_v2_behavior()\n",
        "# import tensorflow.python.ops.gen_logging_ops as logging_ops\n",
        "from tensorflow.python.ops import gen_logging_ops\n",
        "from tensorflow.python.framework import ops as _ops\n",
        "from tensorflow.python.framework import ops as _ops\n",
        "from tensorflow.python.ops import array_ops as _array_ops\n",
        "from tensorflow.python.ops import gen_logging_ops as _gen_logging_ops\n",
        "from tensorflow.python.ops import gen_summary_ops as _gen_summary_ops  # pylint: disable=unused-import\n",
        "from tensorflow.python.ops import summary_op_util as _summary_op_util\n",
        "from tensorflow.python.ops import summary_ops_v2 as _summary_ops_v2"
      ],
      "metadata": {
        "id": "VRyTgAYCg4Et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f29f585-99d4-4942-d587-7bc0d4b422a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_dict_file = 'kb4rec_entity_new.csv'\n",
        "        relation_dict_file = 'Relation2Id_new.csv'\n",
        "        print('-----Loading entity dict-----')\n",
        "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= ',')\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "       # print(self.entity_dict)\n",
        "        self.n_entity = len(self.entity_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= ',')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        with open('label_entity_embedding.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'train_5.csv'\n",
        "        validation_file = 'valid_5.csv'\n",
        "        test_file = 'test_5.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= '\\t')\n",
        "        self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_dict[t] for t in training_df[2]],\n",
        "                                         [self.relation_dict[r] for r in training_df[1]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= '\\t')\n",
        "        self.validation_triples = list(zip([self.entity_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_dict[t] for t in validation_df[2]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[1]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= '\\t')\n",
        "        self.test_triples = list(zip([self.entity_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_dict[t] for t in test_df[2]],\n",
        "                                     [self.relation_dict[r] for r in test_df[1]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "                                                    shape=[kg.n_entity, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity)\n",
        "            \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            \n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_embedding = self.entity_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        entity_norm = np.linalg.norm(entity_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "\n",
        "\n",
        "        with open('entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_embedding)\n",
        "\n",
        "        with open('relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 50\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "bLwsU3g-Hf86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() # tf.summary.merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl10rhKSSYY3",
        "outputId": "7645da7b-15fd-478f-8a7c-84f684eee8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Loading entity dict-----\n",
            "-----Loading relation dict-----\n",
            "#relation: 46\n",
            "-----Loading training triples-----\n",
            "#training triple: 94976\n",
            "-----Loading validation triples-----\n",
            "#validation triple: 95335\n",
            "-----Loading test triples------\n",
            "#test triple: 94976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56788\n",
            "-----Initializing tf graph-----\n",
            "-----Initialization accomplished-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 0]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.525s] #triple: 93952/94976 triple_avg_loss: 1.021276\n",
            "epoch loss: 100519.401\n",
            "cost time: 0.525s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 1]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.475s] #triple: 93952/94976 triple_avg_loss: 0.600755\n",
            "epoch loss: 61756.283\n",
            "cost time: 0.475s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 2]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.464s] #triple: 91904/94976 triple_avg_loss: 0.219801\n",
            "epoch loss: 46992.011\n",
            "cost time: 0.464s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 3]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.446s] #triple: 92928/94976 triple_avg_loss: 0.119293\n",
            "epoch loss: 17608.970\n",
            "cost time: 0.446s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 4]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.451s] #triple: 91904/94976 triple_avg_loss: 0.042916\n",
            "epoch loss: 7356.502\n",
            "cost time: 0.451s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 5]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.450s] #triple: 92928/94976 triple_avg_loss: 0.031423\n",
            "epoch loss: 4271.576\n",
            "cost time: 0.450s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 6]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.458s] #triple: 91904/94976 triple_avg_loss: 0.016110\n",
            "epoch loss: 3688.098\n",
            "cost time: 0.458s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 7]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.463s] #triple: 92928/94976 triple_avg_loss: 0.166007\n",
            "epoch loss: 24296.353\n",
            "cost time: 0.463s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 8]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.466s] #triple: 93952/94976 triple_avg_loss: 0.157739\n",
            "epoch loss: 15862.984\n",
            "cost time: 0.466s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 9]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.430s] #triple: 93952/94976 triple_avg_loss: 0.096340\n",
            "epoch loss: 10258.448\n",
            "cost time: 0.430s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "-----Start evaluation-----\n",
            "[470.754s] #evaluation triple: 94976/94976\n",
            "-----Joining all rank calculator-----\n",
            "-----All rank calculation accomplished-----\n",
            "-----Obtaining evaluation results-----\n",
            "-----Raw-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 1307.698, Hits@10: 0.090\n",
            "-----Tail prediction-----\n",
            "MeanRank: 239.073, Hits@10: 0.373\n",
            "------Average------\n",
            "MeanRank: 773.386, Hits@10: 0.231\n",
            "-----Filter-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 1141.130, Hits@10: 0.097\n",
            "-----Tail prediction-----\n",
            "MeanRank: 238.537, Hits@10: 0.376\n",
            "-----Average-----\n",
            "MeanRank: 689.834, Hits@10: 0.236\n",
            "cost time: 472.208s\n",
            "-----Finish evaluation-----\n",
            "==============================[EPOCH 10]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.461s] #triple: 93952/94976 triple_avg_loss: 0.022317\n",
            "epoch loss: 1930.716\n",
            "cost time: 0.461s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 11]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.447s] #triple: 93952/94976 triple_avg_loss: 0.074099\n",
            "epoch loss: 6695.637\n",
            "cost time: 0.447s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 12]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.472s] #triple: 92928/94976 triple_avg_loss: 0.009418\n",
            "epoch loss: 1277.195\n",
            "cost time: 0.472s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 13]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.478s] #triple: 91904/94976 triple_avg_loss: 0.059264\n",
            "epoch loss: 4327.187\n",
            "cost time: 0.478s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 14]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.480s] #triple: 92928/94976 triple_avg_loss: 0.050277\n",
            "epoch loss: 4221.315\n",
            "cost time: 0.480s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 15]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.488s] #triple: 94976/94976 triple_avg_loss: 0.013507\n",
            "epoch loss: 3576.460\n",
            "cost time: 0.488s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 16]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.458s] #triple: 92928/94976 triple_avg_loss: 0.028306\n",
            "epoch loss: 2959.162\n",
            "cost time: 0.458s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 17]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.448s] #triple: 94976/94976 triple_avg_loss: 0.010003\n",
            "epoch loss: 1004.279\n",
            "cost time: 0.448s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 18]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.446s] #triple: 94976/94976 triple_avg_loss: 0.031138\n",
            "epoch loss: 2671.778\n",
            "cost time: 0.446s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 19]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.469s] #triple: 94976/94976 triple_avg_loss: 0.008672\n",
            "epoch loss: 795.377\n",
            "cost time: 0.470s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "-----Start evaluation-----\n",
            "[428.401s] #evaluation triple: 94976/94976\n",
            "-----Joining all rank calculator-----\n",
            "-----All rank calculation accomplished-----\n",
            "-----Obtaining evaluation results-----\n",
            "-----Raw-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 397.553, Hits@10: 0.208\n",
            "-----Tail prediction-----\n",
            "MeanRank: 59.297, Hits@10: 0.627\n",
            "------Average------\n",
            "MeanRank: 228.425, Hits@10: 0.418\n",
            "-----Filter-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 242.476, Hits@10: 0.262\n",
            "-----Tail prediction-----\n",
            "MeanRank: 58.801, Hits@10: 0.635\n",
            "-----Average-----\n",
            "MeanRank: 150.639, Hits@10: 0.449\n",
            "cost time: 429.859s\n",
            "-----Finish evaluation-----\n",
            "==============================[EPOCH 20]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.446s] #triple: 92928/94976 triple_avg_loss: 0.020817\n",
            "epoch loss: 2204.640\n",
            "cost time: 0.446s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 21]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.472s] #triple: 92928/94976 triple_avg_loss: 0.006206\n",
            "epoch loss: 680.516\n",
            "cost time: 0.472s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 22]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.463s] #triple: 93952/94976 triple_avg_loss: 0.021814\n",
            "epoch loss: 2024.780\n",
            "cost time: 0.464s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 23]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.432s] #triple: 93952/94976 triple_avg_loss: 0.006129\n",
            "epoch loss: 574.627\n",
            "cost time: 0.432s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 24]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.417s] #triple: 90880/94976 triple_avg_loss: 0.004285\n",
            "epoch loss: 663.327\n",
            "cost time: 0.418s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 25]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.427s] #triple: 93952/94976 triple_avg_loss: 0.021284\n",
            "epoch loss: 1765.834\n",
            "cost time: 0.427s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 26]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.474s] #triple: 94976/94976 triple_avg_loss: 0.017839\n",
            "epoch loss: 1675.586\n",
            "cost time: 0.474s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 27]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.452s] #triple: 94976/94976 triple_avg_loss: 0.020556\n",
            "epoch loss: 1648.387\n",
            "cost time: 0.452s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 28]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.456s] #triple: 92928/94976 triple_avg_loss: 0.012258\n",
            "epoch loss: 1404.637\n",
            "cost time: 0.456s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 29]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.452s] #triple: 93952/94976 triple_avg_loss: 0.012817\n",
            "epoch loss: 1394.551\n",
            "cost time: 0.453s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "-----Start evaluation-----\n",
            "[420.769s] #evaluation triple: 94976/94976\n",
            "-----Joining all rank calculator-----\n",
            "-----All rank calculation accomplished-----\n",
            "-----Obtaining evaluation results-----\n",
            "-----Raw-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 275.139, Hits@10: 0.271\n",
            "-----Tail prediction-----\n",
            "MeanRank: 42.468, Hits@10: 0.690\n",
            "------Average------\n",
            "MeanRank: 158.804, Hits@10: 0.480\n",
            "-----Filter-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 121.844, Hits@10: 0.380\n",
            "-----Tail prediction-----\n",
            "MeanRank: 41.983, Hits@10: 0.698\n",
            "-----Average-----\n",
            "MeanRank: 81.914, Hits@10: 0.539\n",
            "cost time: 422.109s\n",
            "-----Finish evaluation-----\n",
            "==============================[EPOCH 30]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.451s] #triple: 93952/94976 triple_avg_loss: 0.006069\n",
            "epoch loss: 613.999\n",
            "cost time: 0.451s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 31]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.490s] #triple: 91904/94976 triple_avg_loss: 0.011525\n",
            "epoch loss: 1161.810\n",
            "cost time: 0.490s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 32]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.488s] #triple: 93952/94976 triple_avg_loss: 0.011944\n",
            "epoch loss: 1083.732\n",
            "cost time: 0.488s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 33]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.437s] #triple: 92928/94976 triple_avg_loss: 0.005391\n",
            "epoch loss: 409.775\n",
            "cost time: 0.438s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 34]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[2.368s] #triple: 92928/94976 triple_avg_loss: 0.010932\n",
            "epoch loss: 1154.079\n",
            "cost time: 2.368s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 35]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.486s] #triple: 92928/94976 triple_avg_loss: 0.004008\n",
            "epoch loss: 426.980\n",
            "cost time: 0.486s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 36]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[1.866s] #triple: 91904/94976 triple_avg_loss: 0.002357\n",
            "epoch loss: 1000.103\n",
            "cost time: 1.866s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 37]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.547s] #triple: 92928/94976 triple_avg_loss: 0.003320\n",
            "epoch loss: 402.625\n",
            "cost time: 0.547s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 38]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.622s] #triple: 92928/94976 triple_avg_loss: 0.002504\n",
            "epoch loss: 314.674\n",
            "cost time: 0.623s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 39]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.635s] #triple: 91904/94976 triple_avg_loss: 0.010924\n",
            "epoch loss: 1006.560\n",
            "cost time: 0.635s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "-----Start evaluation-----\n",
            "[422.525s] #evaluation triple: 94976/94976\n",
            "-----Joining all rank calculator-----\n",
            "-----All rank calculation accomplished-----\n",
            "-----Obtaining evaluation results-----\n",
            "-----Raw-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 245.698, Hits@10: 0.309\n",
            "-----Tail prediction-----\n",
            "MeanRank: 21.188, Hits@10: 0.777\n",
            "------Average------\n",
            "MeanRank: 133.443, Hits@10: 0.543\n",
            "-----Filter-----\n",
            "-----Head prediction-----\n",
            "MeanRank: 92.981, Hits@10: 0.450\n",
            "-----Tail prediction-----\n",
            "MeanRank: 20.706, Hits@10: 0.788\n",
            "-----Average-----\n",
            "MeanRank: 56.843, Hits@10: 0.619\n",
            "cost time: 423.976s\n",
            "-----Finish evaluation-----\n",
            "==============================[EPOCH 40]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.513s] #triple: 91904/94976 triple_avg_loss: 0.012583\n",
            "epoch loss: 290.288\n",
            "cost time: 0.513s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 41]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.872s] #triple: 91904/94976 triple_avg_loss: 0.001737\n",
            "epoch loss: 331.767\n",
            "cost time: 0.872s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 42]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[1.113s] #triple: 93952/94976 triple_avg_loss: 0.003702\n",
            "epoch loss: 223.256\n",
            "cost time: 1.114s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 43]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[1.059s] #triple: 89856/94976 triple_avg_loss: 0.009376\n",
            "epoch loss: 416.282\n",
            "cost time: 1.059s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 44]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[3.202s] #triple: 91904/94976 triple_avg_loss: 0.011155\n",
            "epoch loss: 926.808\n",
            "cost time: 3.202s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 45]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.583s] #triple: 94976/94976 triple_avg_loss: 0.002436\n",
            "epoch loss: 231.705\n",
            "cost time: 0.584s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 46]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.710s] #triple: 92928/94976 triple_avg_loss: 0.001621\n",
            "epoch loss: 194.468\n",
            "cost time: 0.710s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 47]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[1.451s] #triple: 91904/94976 triple_avg_loss: 0.001596\n",
            "epoch loss: 188.465\n",
            "cost time: 1.451s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 48]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[13.568s] #triple: 91904/94976 triple_avg_loss: 0.001314\n",
            "epoch loss: 181.909\n",
            "cost time: 13.568s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "==============================[EPOCH 49]==============================\n",
            "-----Start training-----\n",
            "-----Constructing training batches-----\n",
            "[0.638s] #triple: 91904/94976 triple_avg_loss: 0.001717\n",
            "epoch loss: 235.613\n",
            "cost time: 0.639s\n",
            "-----Finish training-----\n",
            "-----Check norm-----\n",
            "-----Start evaluation-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1B_3yO5ctiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 100"
      ],
      "metadata": {
        "id": "kjLfqghCbLyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_dict_file = 'kb4rec_entity_new.csv'\n",
        "        relation_dict_file = 'Relation2Id_new.csv'\n",
        "        print('-----Loading entity dict-----')\n",
        "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= ',')\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "       # print(self.entity_dict)\n",
        "        self.n_entity = len(self.entity_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= ',')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        with open('label_entity_embedding.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'train_5.csv'\n",
        "        validation_file = 'valid_5.csv'\n",
        "        test_file = 'test_5.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= '\\t')\n",
        "        self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_dict[t] for t in training_df[2]],\n",
        "                                         [self.relation_dict[r] for r in training_df[1]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= '\\t')\n",
        "        self.validation_triples = list(zip([self.entity_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_dict[t] for t in validation_df[2]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[1]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= '\\t')\n",
        "        self.test_triples = list(zip([self.entity_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_dict[t] for t in test_df[2]],\n",
        "                                     [self.relation_dict[r] for r in test_df[1]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "                                                    shape=[kg.n_entity, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity)\n",
        "            \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_embedding = self.entity_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        entity_norm = np.linalg.norm(entity_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_embedding)\n",
        "\n",
        "        with open('relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 100\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "0sXTwu_3Wn6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "N4tcUWrYc4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trimmemd Network based on analysis"
      ],
      "metadata": {
        "id": "2hnvnAxybtMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 50"
      ],
      "metadata": {
        "id": "PmoZe6nTbzV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_dict_file = 'entity2id_case2 - entity2id_case2.csv'\n",
        "        relation_dict_file = 'analyis_relation2Id_new.csv'\n",
        "        print('-----Loading entity dict-----')\n",
        "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= ',')\n",
        "        \n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "       # print(self.entity_dict)\n",
        "        self.n_entity = len(self.entity_dict)\n",
        "        print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= ',')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        #self.n_relation = len(self.relation_dict)\n",
        "        self.n_relation = len(relation_df)\n",
        "        \n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        print(relation_df)\n",
        "        with open('label_entity_embedding.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'train_5_case2 - train_5_case2.csv'\n",
        "        validation_file = 'val_5_case2 - val_5_case2.csv'\n",
        "        test_file = 'test_5_case2 - test_5_case2.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "\n",
        "       #training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_dict[t] for t in training_df[2]],\n",
        "                                         [self.relation_dict[r] for r in training_df[1]]))\n",
        "        \n",
        "\n",
        "          # self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "          #                                 [self.entity_dict[t] for t in training_df[2]],\n",
        "          #                                 [self.relation_dict[r] for r in training_df[1]]))\n",
        "\n",
        "\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        \n",
        "        print('-----Loading validation triples-----')\n",
        "        #validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_dict[t] for t in validation_df[2]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[1]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        \n",
        "        \n",
        "        #test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_dict[t] for t in test_df[2]],\n",
        "                                     [self.relation_dict[r] for r in test_df[1]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "                                                    shape=[kg.n_entity, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity)\n",
        "            \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            print('-----Initializing embedding -----')\n",
        "            tf.compat.v1.global_variables_initializer()      \n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "      \n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "          \n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_embedding = self.entity_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        entity_norm = np.linalg.norm(entity_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('entity_embedding_case2_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_embedding)\n",
        "\n",
        "        with open('relation_embedding_case2_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 50\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =128\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "XlmII8PnbvUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "lS6RGYu4c5Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 100"
      ],
      "metadata": {
        "id": "s6I5wlEyb2eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_dict_file = 'entity2id_case2 - entity2id_case2.csv'\n",
        "        relation_dict_file = 'analyis_relation2Id_new.csv'\n",
        "        print('-----Loading entity dict-----')\n",
        "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= ',')\n",
        "        \n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "       # print(self.entity_dict)\n",
        "        self.n_entity = len(self.entity_dict)\n",
        "        print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= ',')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        #self.n_relation = len(self.relation_dict)\n",
        "        self.n_relation = len(relation_df)\n",
        "        \n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        print(relation_df)\n",
        "        with open('label_entity_embedding.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'train_5_case2 - train_5_case2.csv'\n",
        "        validation_file = 'val_5_case2 - val_5_case2.csv'\n",
        "        test_file = 'test_5_case2 - test_5_case2.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "\n",
        "       #training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_dict[t] for t in training_df[2]],\n",
        "                                         [self.relation_dict[r] for r in training_df[1]]))\n",
        "        \n",
        "\n",
        "          # self.training_triples = list(zip([self.entity_dict[h] for h in training_df[0]],\n",
        "          #                                 [self.entity_dict[t] for t in training_df[2]],\n",
        "          #                                 [self.relation_dict[r] for r in training_df[1]]))\n",
        "\n",
        "\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        \n",
        "        print('-----Loading validation triples-----')\n",
        "        #validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_dict[t] for t in validation_df[2]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[1]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        \n",
        "        \n",
        "        #test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_dict[t] for t in test_df[2]],\n",
        "                                     [self.relation_dict[r] for r in test_df[1]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "                                                    shape=[kg.n_entity, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity)\n",
        "            \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            print('-----Initializing embedding -----')\n",
        "            tf.compat.v1.global_variables_initializer()      \n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #print(\"check2\")\n",
        "    \n",
        "            # batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.merge],\n",
        "            #                          feed_dict={self.triple_pos: batch_pos,\n",
        "            #                                     self.triple_neg: batch_neg,\n",
        "            #                                     self.margin: [self.margin_value] * len(batch_pos)})\n",
        "\n",
        "            #print(summary)\n",
        "            #summary = tf.compat.v1.summary.merge(summary)\n",
        "            #print(self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_embedding = self.entity_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        entity_norm = np.linalg.norm(entity_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('entity_embedding_case2_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_embedding)\n",
        "\n",
        "        with open('relation_embedding_case2_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 100\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =128\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "yyyMVDmab3lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "bNNvhcxpc5y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# People Based KG"
      ],
      "metadata": {
        "id": "CHi0eE3KcAdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total Network"
      ],
      "metadata": {
        "id": "_7WFtiO6cOrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 50"
      ],
      "metadata": {
        "id": "DqC33dL_cTXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity_head = 0\n",
        "        self.n_entity_tail = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_head_dict_file = 'entity2id_head.txt'\n",
        "        entity_tail_dict_file = 'entity2id_tail.txt'\n",
        "        relation_dict_file = 'relation2id.txt'\n",
        "        print('-----Loading entity dict-----')\n",
        "        #entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= '\\t')\n",
        "        entity_head_df = pd.read_table(os.path.join(self.data_dir, entity_head_dict_file), header=None, sep= '\\t')\n",
        "        entity_tail_df = pd.read_table(os.path.join(self.data_dir, entity_tail_dict_file), header=None, sep= '\\t')        \n",
        "        print(entity_head_df.head())\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        #self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "        self.entity_head_dict = dict(zip(entity_head_df[0], entity_head_df[1]))\n",
        "        self.entity_tail_dict = dict(zip(entity_tail_df[0], entity_tail_df[1]))        \n",
        "        self.n_entity_head = len(self.entity_head_dict)\n",
        "        self.n_entity_tail = len(self.entity_tail_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        \n",
        "        print(self.n_entity_head)\n",
        "        print(self.n_entity_tail)\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= '\\t')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        print(self.relation_dict)\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print(self.n_relation)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        # with open('label_entity_embedding.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'imdb30_train - imdb30_train.csv'\n",
        "        validation_file = 'imdb30_valid - imdb30_valid.csv'\n",
        "        test_file = 'imdb30_test - imdb30_test.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        print(training_df.head())\n",
        "        self.training_triples = list(zip([self.entity_head_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_tail_dict[t] for t in training_df[1]],\n",
        "                                         [self.relation_dict[r] for r in training_df[2]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_head_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_tail_dict[t] for t in validation_df[1]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[2]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_head_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_tail_dict[t] for t in test_df[1]],\n",
        "                                     [self.relation_dict[r] for r in test_df[2]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        print(\"---------check-------------------\")\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_head_embedding = tf.compat.v1.get_variable(name='entity_head',\n",
        "                                                    shape=[kg.n_entity_head, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            self.entity_tail_embedding = tf.compat.v1.get_variable(name='entity_tail',\n",
        "                                                    shape=[kg.n_entity_tail, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            # self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "            #                                         shape=[kg.n_entity, self.embedding_dim],\n",
        "            #                                         initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "            #                                                                                   maxval=bound))\n",
        "            print(\"----------check2--------------------\")\n",
        "            tf.summary.histogram('self.entity_head_embedding.op.name', self.entity_head_embedding)\n",
        "            tf.summary.histogram('self.entity_tail_embedding.op.name', self.entity_tail_embedding)\n",
        "            \n",
        "            #tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity_head)\n",
        "            print(kg.n_entity_tail)           \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            #self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.entity_head_embedding = tf.nn.l2_normalize(self.entity_head_embedding, dim=1)\n",
        "            self.entity_tail_embedding = tf.nn.l2_normalize(self.entity_tail_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_head_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_head_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_head_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_tail_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_head_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_tail_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #print(\"check2\")\n",
        "    \n",
        "            # batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.merge],\n",
        "            #                          feed_dict={self.triple_pos: batch_pos,\n",
        "            #                                     self.triple_neg: batch_neg,\n",
        "            #                                     self.margin: [self.margin_value] * len(batch_pos)})\n",
        "\n",
        "            #print(summary)\n",
        "            #summary = tf.compat.v1.summary.merge(summary)\n",
        "            #print(self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_head_embedding = self.entity_head_embedding.eval(session=session)\n",
        "        entity_tail_embedding = self.entity_tail_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        head_entity_norm = np.linalg.norm(entity_head_embedding, ord=2, axis=1)\n",
        "        tail_entity_norm = np.linalg.norm(entity_tail_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('imdb30_c1_head_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_head_embedding)\n",
        "\n",
        "        with open('imdb30_c1_tail_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_tail_embedding)\n",
        "\n",
        "        with open('imdb30_c1_relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description='TransE')\n",
        "    # parser.add_argument('--data_dir', type=str, default='../data/FB15k/')\n",
        "    # parser.add_argument('--embedding_dim', type=int, default=200)\n",
        "    # parser.add_argument('--margin_value', type=float, default=1.0)\n",
        "    # parser.add_argument('--score_func', type=str, default='L1')\n",
        "    # parser.add_argument('--batch_size', type=int, default=4800)\n",
        "    # parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "    # parser.add_argument('--n_generator', type=int, default=24)\n",
        "    # parser.add_argument('--n_rank_calculator', type=int, default=24)\n",
        "    # parser.add_argument('--ckpt_dir', type=str, default='../ckpt/')\n",
        "    # parser.add_argument('--summary_dir', type=str, default='../summary/')\n",
        "    # parser.add_argument('--max_epoch', type=int, default=500)\n",
        "    # parser.add_argument('--eval_freq', type=int, default=10)\n",
        "    #args = parser.parse_args()\n",
        "    #print(args)\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 50\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "KjQT9gVncCPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "_CIzB__9c6dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 100"
      ],
      "metadata": {
        "id": "M_D2f7b_cWgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity_head = 0\n",
        "        self.n_entity_tail = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_head_dict_file = 'entity2id_head.txt'\n",
        "        entity_tail_dict_file = 'entity2id_tail.txt'\n",
        "        relation_dict_file = 'relation2id.txt'\n",
        "        print('-----Loading entity dict-----')\n",
        "        #entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= '\\t')\n",
        "        entity_head_df = pd.read_table(os.path.join(self.data_dir, entity_head_dict_file), header=None, sep= '\\t')\n",
        "        entity_tail_df = pd.read_table(os.path.join(self.data_dir, entity_tail_dict_file), header=None, sep= '\\t')        \n",
        "        print(entity_head_df.head())\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        #self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "        self.entity_head_dict = dict(zip(entity_head_df[0], entity_head_df[1]))\n",
        "        self.entity_tail_dict = dict(zip(entity_tail_df[0], entity_tail_df[1]))        \n",
        "        self.n_entity_head = len(self.entity_head_dict)\n",
        "        self.n_entity_tail = len(self.entity_tail_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        \n",
        "        print(self.n_entity_head)\n",
        "        print(self.n_entity_tail)\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= '\\t')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        print(self.relation_dict)\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print(self.n_relation)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        # with open('label_entity_embedding.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'imdb30_train - imdb30_train.csv'\n",
        "        validation_file = 'imdb30_valid - imdb30_valid.csv'\n",
        "        test_file = 'imdb30_test - imdb30_test.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        print(training_df.head())\n",
        "        self.training_triples = list(zip([self.entity_head_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_tail_dict[t] for t in training_df[1]],\n",
        "                                         [self.relation_dict[r] for r in training_df[2]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_head_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_tail_dict[t] for t in validation_df[1]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[2]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_head_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_tail_dict[t] for t in test_df[1]],\n",
        "                                     [self.relation_dict[r] for r in test_df[2]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        print(\"---------check-------------------\")\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_head_embedding = tf.compat.v1.get_variable(name='entity_head',\n",
        "                                                    shape=[kg.n_entity_head, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            self.entity_tail_embedding = tf.compat.v1.get_variable(name='entity_tail',\n",
        "                                                    shape=[kg.n_entity_tail, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            # self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "            #                                         shape=[kg.n_entity, self.embedding_dim],\n",
        "            #                                         initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "            #                                                                                   maxval=bound))\n",
        "            print(\"----------check2--------------------\")\n",
        "            tf.summary.histogram('self.entity_head_embedding.op.name', self.entity_head_embedding)\n",
        "            tf.summary.histogram('self.entity_tail_embedding.op.name', self.entity_tail_embedding)\n",
        "            \n",
        "            #tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity_head)\n",
        "            print(kg.n_entity_tail)           \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            #self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.entity_head_embedding = tf.nn.l2_normalize(self.entity_head_embedding, dim=1)\n",
        "            self.entity_tail_embedding = tf.nn.l2_normalize(self.entity_tail_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_head_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_head_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_head_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_tail_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_head_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_tail_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #print(\"check2\")\n",
        "    \n",
        "            # batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.merge],\n",
        "            #                          feed_dict={self.triple_pos: batch_pos,\n",
        "            #                                     self.triple_neg: batch_neg,\n",
        "            #                                     self.margin: [self.margin_value] * len(batch_pos)})\n",
        "\n",
        "            #print(summary)\n",
        "            #summary = tf.compat.v1.summary.merge(summary)\n",
        "            #print(self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_head_embedding = self.entity_head_embedding.eval(session=session)\n",
        "        entity_tail_embedding = self.entity_tail_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        head_entity_norm = np.linalg.norm(entity_head_embedding, ord=2, axis=1)\n",
        "        tail_entity_norm = np.linalg.norm(entity_tail_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('imdb30_c1_head_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_head_embedding)\n",
        "\n",
        "        with open('imdb30_c1_tail_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_tail_embedding)\n",
        "\n",
        "        with open('imdb30_c1_relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 100\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "oKp0ioiRcXii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "_jucLo14c7IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trimmed Network based on analysis"
      ],
      "metadata": {
        "id": "JMBZ-KFRcd8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 50"
      ],
      "metadata": {
        "id": "MIv7hU2nchf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity_head = 0\n",
        "        self.n_entity_tail = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_head_dict_file = 'entity2id_head.txt'\n",
        "        entity_tail_dict_file = 'entity2id_tail.txt'\n",
        "        relation_dict_file = 'relation2id.txt'\n",
        "        print('-----Loading entity dict-----')\n",
        "        #entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= '\\t')\n",
        "        entity_head_df = pd.read_table(os.path.join(self.data_dir, entity_head_dict_file), header=None, sep= '\\t')\n",
        "        entity_tail_df = pd.read_table(os.path.join(self.data_dir, entity_tail_dict_file), header=None, sep= '\\t')        \n",
        "        print(entity_head_df.head())\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        #self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "        self.entity_head_dict = dict(zip(entity_head_df[0], entity_head_df[1]))\n",
        "        self.entity_tail_dict = dict(zip(entity_tail_df[0], entity_tail_df[1]))        \n",
        "        self.n_entity_head = len(self.entity_head_dict)\n",
        "        self.n_entity_tail = len(self.entity_tail_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        \n",
        "        print(self.n_entity_head)\n",
        "        print(self.n_entity_tail)\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= '\\t')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        print(self.relation_dict)\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print(self.n_relation)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        # with open('label_entity_embedding.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'imdb30_train - imdb30_train.csv'\n",
        "        validation_file = 'imdb30_valid - imdb30_valid.csv'\n",
        "        test_file = 'imdb30_test - imdb30_test.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        print(training_df.head())\n",
        "        self.training_triples = list(zip([self.entity_head_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_tail_dict[t] for t in training_df[1]],\n",
        "                                         [self.relation_dict[r] for r in training_df[2]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_head_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_tail_dict[t] for t in validation_df[1]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[2]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_head_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_tail_dict[t] for t in test_df[1]],\n",
        "                                     [self.relation_dict[r] for r in test_df[2]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        print(\"---------check-------------------\")\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_head_embedding = tf.compat.v1.get_variable(name='entity_head',\n",
        "                                                    shape=[kg.n_entity_head, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            self.entity_tail_embedding = tf.compat.v1.get_variable(name='entity_tail',\n",
        "                                                    shape=[kg.n_entity_tail, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            # self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "            #                                         shape=[kg.n_entity, self.embedding_dim],\n",
        "            #                                         initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "            #                                                                                   maxval=bound))\n",
        "            print(\"----------check2--------------------\")\n",
        "            tf.summary.histogram('self.entity_head_embedding.op.name', self.entity_head_embedding)\n",
        "            tf.summary.histogram('self.entity_tail_embedding.op.name', self.entity_tail_embedding)\n",
        "            \n",
        "            #tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity_head)\n",
        "            print(kg.n_entity_tail)           \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            #self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.entity_head_embedding = tf.nn.l2_normalize(self.entity_head_embedding, dim=1)\n",
        "            self.entity_tail_embedding = tf.nn.l2_normalize(self.entity_tail_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_head_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_head_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_head_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_tail_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_head_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_tail_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #print(\"check2\")\n",
        "    \n",
        "            # batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.merge],\n",
        "            #                          feed_dict={self.triple_pos: batch_pos,\n",
        "            #                                     self.triple_neg: batch_neg,\n",
        "            #                                     self.margin: [self.margin_value] * len(batch_pos)})\n",
        "\n",
        "            #print(summary)\n",
        "            #summary = tf.compat.v1.summary.merge(summary)\n",
        "            #print(self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_head_embedding = self.entity_head_embedding.eval(session=session)\n",
        "        entity_tail_embedding = self.entity_tail_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        head_entity_norm = np.linalg.norm(entity_head_embedding, ord=2, axis=1)\n",
        "        tail_entity_norm = np.linalg.norm(entity_tail_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('imdb30_c1_head_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_head_embedding)\n",
        "\n",
        "        with open('imdb30_c1_tail_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_tail_embedding)\n",
        "\n",
        "        with open('imdb30_c1_relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description='TransE')\n",
        "    # parser.add_argument('--data_dir', type=str, default='../data/FB15k/')\n",
        "    # parser.add_argument('--embedding_dim', type=int, default=200)\n",
        "    # parser.add_argument('--margin_value', type=float, default=1.0)\n",
        "    # parser.add_argument('--score_func', type=str, default='L1')\n",
        "    # parser.add_argument('--batch_size', type=int, default=4800)\n",
        "    # parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "    # parser.add_argument('--n_generator', type=int, default=24)\n",
        "    # parser.add_argument('--n_rank_calculator', type=int, default=24)\n",
        "    # parser.add_argument('--ckpt_dir', type=str, default='../ckpt/')\n",
        "    # parser.add_argument('--summary_dir', type=str, default='../summary/')\n",
        "    # parser.add_argument('--max_epoch', type=int, default=500)\n",
        "    # parser.add_argument('--eval_freq', type=int, default=10)\n",
        "    #args = parser.parse_args()\n",
        "    #print(args)\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 50\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "R5Un4bIycfPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "aOYLJicyc79s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 100"
      ],
      "metadata": {
        "id": "rhlfcKkycnRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.entity_dict = {}\n",
        "        self.entities = []\n",
        "        self.relation_dict = {}\n",
        "        self.n_entity_head = 0\n",
        "        self.n_entity_tail = 0\n",
        "        self.n_relation = 0\n",
        "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
        "        self.validation_triples = []\n",
        "        self.test_triples = []\n",
        "        self.n_training_triple = 0\n",
        "        self.n_validation_triple = 0\n",
        "        self.n_test_triple = 0\n",
        "        '''load dicts and triples'''\n",
        "        self.load_dicts()\n",
        "        self.load_triples()\n",
        "        '''construct pools after loading'''\n",
        "        self.training_triple_pool = set(self.training_triples)\n",
        "        self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
        "\n",
        "    def load_dicts(self):\n",
        "        entity_head_dict_file = 'entity2id_head.txt'\n",
        "        entity_tail_dict_file = 'entity2id_tail.txt'\n",
        "        relation_dict_file = 'relation2id.txt'\n",
        "        print('-----Loading entity dict-----')\n",
        "        #entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None, sep= '\\t')\n",
        "        entity_head_df = pd.read_table(os.path.join(self.data_dir, entity_head_dict_file), header=None, sep= '\\t')\n",
        "        entity_tail_df = pd.read_table(os.path.join(self.data_dir, entity_tail_dict_file), header=None, sep= '\\t')        \n",
        "        print(entity_head_df.head())\n",
        "        #print(entity_df)\n",
        "        #print(entity_df.info())\n",
        "        #print(entity_df[0])\n",
        "        #print(entity_df[1])\n",
        "        #print(dict(entity_df[0], entity_df[1]))\n",
        "        #self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
        "        self.entity_head_dict = dict(zip(entity_head_df[0], entity_head_df[1]))\n",
        "        self.entity_tail_dict = dict(zip(entity_tail_df[0], entity_tail_df[1]))        \n",
        "        self.n_entity_head = len(self.entity_head_dict)\n",
        "        self.n_entity_tail = len(self.entity_tail_dict)\n",
        "        #print(self.n_entity)\n",
        "        self.entities = list(self.entity_dict.values())\n",
        "        #print(self.entities)\n",
        "       # print('#entity: {}'.format(self.n_entity))\n",
        "        \n",
        "        print(self.n_entity_head)\n",
        "        print(self.n_entity_tail)\n",
        "        print('-----Loading relation dict-----')\n",
        "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None, sep= '\\t')\n",
        "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
        "        print(self.relation_dict)\n",
        "        self.n_relation = len(self.relation_dict)\n",
        "        print(self.n_relation)\n",
        "        print('#relation: {}'.format(self.n_relation))\n",
        "        # with open('label_entity_embedding.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(self.entity_dict)\n",
        "\n",
        "\n",
        "    def load_triples(self):\n",
        "        training_file = 'imdb30_train - imdb30_train.csv'\n",
        "        validation_file = 'imdb30_valid - imdb30_valid.csv'\n",
        "        test_file = 'imdb30_test - imdb30_test.csv'\n",
        "        print('-----Loading training triples-----')\n",
        "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None, sep= ',')\n",
        "        print(training_df.head())\n",
        "        self.training_triples = list(zip([self.entity_head_dict[h] for h in training_df[0]],\n",
        "                                         [self.entity_tail_dict[t] for t in training_df[1]],\n",
        "                                         [self.relation_dict[r] for r in training_df[2]]))\n",
        "        self.n_training_triple = len(self.training_triples)\n",
        "        print('#training triple: {}'.format(self.n_training_triple))\n",
        "        print('-----Loading validation triples-----')\n",
        "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None, sep= ',')\n",
        "        self.validation_triples = list(zip([self.entity_head_dict[h] for h in validation_df[0]],\n",
        "                                           [self.entity_tail_dict[t] for t in validation_df[1]],\n",
        "                                           [self.relation_dict[r] for r in validation_df[2]]))\n",
        "        self.n_validation_triple = len(self.validation_triples)\n",
        "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
        "        print('-----Loading test triples------')\n",
        "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None, sep= ',')\n",
        "        self.test_triples = list(zip([self.entity_head_dict[h] for h in test_df[0]],\n",
        "                                     [self.entity_tail_dict[t] for t in test_df[1]],\n",
        "                                     [self.relation_dict[r] for r in test_df[2]]))\n",
        "        self.n_test_triple = len(self.test_triples)\n",
        "        print('#test triple: {}'.format(self.n_test_triple))\n",
        "\n",
        "    def next_raw_batch(self, batch_size):\n",
        "        rand_idx = np.random.permutation(self.n_training_triple)\n",
        "        start = 0\n",
        "        while start < self.n_training_triple:\n",
        "            end = min(start + batch_size, self.n_training_triple)\n",
        "            yield [self.training_triples[i] for i in rand_idx[start:end]]\n",
        "            start = end\n",
        "\n",
        "    def generate_training_batch(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            raw_batch = in_queue.get()\n",
        "            if raw_batch is None:\n",
        "                return\n",
        "            else:\n",
        "                batch_pos = raw_batch\n",
        "                batch_neg = []\n",
        "                corrupt_head_prob = np.random.binomial(1, 0.5)\n",
        "                for head, tail, relation in batch_pos:\n",
        "                    head_neg = head\n",
        "                    tail_neg = tail\n",
        "                    while True:\n",
        "                        if corrupt_head_prob:\n",
        "                            head_neg = random.choice(self.entities)\n",
        "                        else:\n",
        "                            tail_neg = random.choice(self.entities)\n",
        "                        if (head_neg, tail_neg, relation) not in self.training_triple_pool:\n",
        "                            break\n",
        "                    batch_neg.append((head_neg, tail_neg, relation))\n",
        "                out_queue.put((batch_pos, batch_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import multiprocessing as mp\n",
        "import csv\n",
        "#from dataset import KnowledgeGraph\n",
        "\n",
        "\n",
        "class TransE:\n",
        "    def __init__(self, kg: KnowledgeGraph,\n",
        "                 embedding_dim, margin_value, score_func,\n",
        "                 batch_size, learning_rate, n_generator, n_rank_calculator):\n",
        "        self.kg = kg\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin_value = margin_value\n",
        "        self.score_func = score_func\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_generator = n_generator\n",
        "        self.n_rank_calculator = n_rank_calculator\n",
        "        '''ops for training'''\n",
        "        self.triple_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.triple_neg = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
        "        self.margin = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.train_op = None\n",
        "        #self.train_op = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])\n",
        "        self.loss = None\n",
        "        self.global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        self.merge = None\n",
        "        '''ops for evaluation'''\n",
        "        self.eval_triple = tf.compat.v1.placeholder(dtype=tf.int32, shape=[3])\n",
        "        self.idx_head_prediction = None\n",
        "        self.idx_tail_prediction = None\n",
        "        '''embeddings'''\n",
        "        bound = 6 / math.sqrt(self.embedding_dim)\n",
        "        print(\"---------check-------------------\")\n",
        "        with tf.compat.v1.variable_scope('embedding'):\n",
        "            self.entity_head_embedding = tf.compat.v1.get_variable(name='entity_head',\n",
        "                                                    shape=[kg.n_entity_head, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            self.entity_tail_embedding = tf.compat.v1.get_variable(name='entity_tail',\n",
        "                                                    shape=[kg.n_entity_tail, self.embedding_dim],\n",
        "                                                    initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                              maxval=bound))\n",
        "            # self.entity_embedding = tf.compat.v1.get_variable(name='entity',\n",
        "            #                                         shape=[kg.n_entity, self.embedding_dim],\n",
        "            #                                         initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "            #                                                                                   maxval=bound))\n",
        "            print(\"----------check2--------------------\")\n",
        "            tf.summary.histogram('self.entity_head_embedding.op.name', self.entity_head_embedding)\n",
        "            tf.summary.histogram('self.entity_tail_embedding.op.name', self.entity_tail_embedding)\n",
        "            \n",
        "            #tf.summary.histogram('self.entity_embedding.op.name', self.entity_embedding)\n",
        "            #tf.summary.histogram(name=self.entity_embedding.op.name, values=self.entity_embedding)\n",
        "\n",
        "            print(kg.n_entity_head)\n",
        "            print(kg.n_entity_tail)           \n",
        "            \n",
        "            #tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))\n",
        "            self.relation_embedding = tf.compat.v1.get_variable(name='relation',\n",
        "                                                      shape=[kg.n_relation, self.embedding_dim],\n",
        "                                                      initializer=tf.random_uniform_initializer(minval=-bound,\n",
        "                                                                                                maxval=bound))\n",
        "            tf.summary.histogram('self.relation_embedding.op.name', self.relation_embedding)\n",
        "            #tf.summary.histogram(name=self.relation_embedding.op.name, values=self.relation_embedding)\n",
        "        self.build_graph()\n",
        "        self.build_eval_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.name_scope('normalization'):\n",
        "            #self.entity_embedding = tf.nn.l2_normalize(self.entity_embedding, dim=1)\n",
        "            self.entity_head_embedding = tf.nn.l2_normalize(self.entity_head_embedding, dim=1)\n",
        "            self.entity_tail_embedding = tf.nn.l2_normalize(self.entity_tail_embedding, dim=1)\n",
        "            self.relation_embedding = tf.nn.l2_normalize(self.relation_embedding, dim=1)\n",
        "        with tf.name_scope('training'):\n",
        "            distance_pos, distance_neg = self.infer(self.triple_pos, self.triple_neg)\n",
        "            self.loss = self.calculate_loss(distance_pos, distance_neg, self.margin)\n",
        "            #tf.summary.scalar(name=self.loss.op.name, tensor=self.loss)\n",
        "            tf.summary.scalar('self.loss.op.name', self.loss)\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all()\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key=_ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            #self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES, scope=None, name=None)\n",
        "            self.merge = tf.compat.v1.summary.merge_all(key = _ops.GraphKeys.SUMMARIES)\n",
        " \n",
        "            tf.compat.v1.summary.merge_all()\n",
        "            #tf.merge_all_summaries()\n",
        "\n",
        "    def build_eval_graph(self):\n",
        "        with tf.name_scope('evaluation'):\n",
        "            self.idx_head_prediction, self.idx_tail_prediction = self.evaluate(self.eval_triple)\n",
        "\n",
        "    def infer(self, triple_pos, triple_neg):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head_pos = tf.nn.embedding_lookup(self.entity_head_embedding, triple_pos[:, 0])\n",
        "            tail_pos = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_pos[:, 1])\n",
        "            relation_pos = tf.nn.embedding_lookup(self.relation_embedding, triple_pos[:, 2])\n",
        "            head_neg = tf.nn.embedding_lookup(self.entity_head_embedding, triple_neg[:, 0])\n",
        "            tail_neg = tf.nn.embedding_lookup(self.entity_tail_embedding, triple_neg[:, 1])\n",
        "            relation_neg = tf.nn.embedding_lookup(self.relation_embedding, triple_neg[:, 2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_pos = head_pos + relation_pos - tail_pos\n",
        "            distance_neg = head_neg + relation_neg - tail_neg\n",
        "        return distance_pos, distance_neg\n",
        "\n",
        "    def calculate_loss(self, distance_pos, distance_neg, margin):\n",
        "        with tf.name_scope('loss'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                score_pos = tf.reduce_sum(tf.abs(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.abs(distance_neg), axis=1)\n",
        "            else:  # L2 score\n",
        "                score_pos = tf.reduce_sum(tf.square(distance_pos), axis=1)\n",
        "                score_neg = tf.reduce_sum(tf.square(distance_neg), axis=1)\n",
        "            loss = tf.reduce_sum(tf.nn.relu(margin + score_pos - score_neg), name='max_margin_loss')\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, eval_triple):\n",
        "        with tf.name_scope('lookup'):\n",
        "            head = tf.nn.embedding_lookup(self.entity_head_embedding, eval_triple[0])\n",
        "            tail = tf.nn.embedding_lookup(self.entity_tail_embedding, eval_triple[1])\n",
        "            relation = tf.nn.embedding_lookup(self.relation_embedding, eval_triple[2])\n",
        "        with tf.name_scope('link'):\n",
        "            distance_head_prediction = self.entity_head_embedding + relation - tail\n",
        "            distance_tail_prediction = head + relation - self.entity_tail_embedding\n",
        "        with tf.name_scope('rank'):\n",
        "            if self.score_func == 'L1':  # L1 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.abs(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "            else:  # L2 score\n",
        "                _, idx_head_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_head_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_head)\n",
        "                _, idx_tail_prediction = tf.nn.top_k(tf.reduce_sum(tf.square(distance_tail_prediction), axis=1),\n",
        "                                                     k=self.kg.n_entity_tail)\n",
        "        return idx_head_prediction, idx_tail_prediction\n",
        "\n",
        "    def launch_training(self, session, summary_writer):\n",
        "        raw_batch_queue = mp.Queue()\n",
        "        training_batch_queue = mp.Queue()\n",
        "        for _ in range(self.n_generator):\n",
        "            mp.Process(target=self.kg.generate_training_batch, kwargs={'in_queue': raw_batch_queue,\n",
        "                                                                       'out_queue': training_batch_queue}).start()\n",
        "        print('-----Start training-----')\n",
        "        start = timeit.default_timer()\n",
        "        n_batch = 0\n",
        "        for raw_batch in self.kg.next_raw_batch(self.batch_size):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            n_batch += 1\n",
        "        for _ in range(self.n_generator):\n",
        "            raw_batch_queue.put(raw_batch)\n",
        "            #raw_batch_queue.put(None)\n",
        "        print('-----Constructing training batches-----')\n",
        "        epoch_loss = 0\n",
        "        n_used_triple = 0\n",
        "        #print(n_batch)\n",
        "        # print(\"merge!!!!!!!!!!!:\",self.merge)\n",
        "        # print(\"loss!!!!!!!!!!!:\",self.loss)\n",
        "        # print(\"train_op!!!!!!!!!!!:\",self.train_op)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            batch_pos, batch_neg = training_batch_queue.get()\n",
        "            #print(\"check1\")\n",
        "            batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.margin ],\n",
        "                                                 feed_dict={self.triple_pos: batch_pos,\n",
        "                                                            self.triple_neg: batch_neg,\n",
        "                                                            self.margin: [self.margin_value] * len(batch_pos)})\n",
        "            #print(\"check2\")\n",
        "    \n",
        "            # batch_loss, _, summary = session.run(fetches=[self.loss, self.train_op, self.merge],\n",
        "            #                          feed_dict={self.triple_pos: batch_pos,\n",
        "            #                                     self.triple_neg: batch_neg,\n",
        "            #                                     self.margin: [self.margin_value] * len(batch_pos)})\n",
        "\n",
        "            #print(summary)\n",
        "            #summary = tf.compat.v1.summary.merge(summary)\n",
        "            #print(self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(tf.compat.v1.summary.merge(summary), global_step=self.global_step.eval(session=session))\n",
        "            summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #summary_writer.add_summary(summary, global_step=self.global_step.eval(session=session))\n",
        "            #print(\"check3\")\n",
        "            epoch_loss += batch_loss\n",
        "            n_used_triple += len(batch_pos)\n",
        "            #print(\"check4\")  \n",
        "            #print(\"batch_op\")\n",
        "            print('[{:.3f}s] #triple: {}/{} triple_avg_loss: {:.6f}'.format(timeit.default_timer() - start,\n",
        "                                                                            n_used_triple,\n",
        "                                                                            self.kg.n_training_triple,\n",
        "                                                                            batch_loss / len(batch_pos)), end='\\r')\n",
        "        print()\n",
        "        print('epoch loss: {:.3f}'.format(epoch_loss))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish training-----')\n",
        "        self.check_norm(session=session)\n",
        "        #return self.kg.n_training_triple\n",
        "\n",
        "    def launch_evaluation(self, session):\n",
        "        eval_result_queue = mp.JoinableQueue()\n",
        "        rank_result_queue = mp.Queue()\n",
        "        print('-----Start evaluation-----')\n",
        "        start = timeit.default_timer()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            mp.Process(target=self.calculate_rank, kwargs={'in_queue': eval_result_queue,\n",
        "                                                           'out_queue': rank_result_queue}).start()\n",
        "        n_used_eval_triple = 0\n",
        "        for eval_triple in self.kg.test_triples:\n",
        "            idx_head_prediction, idx_tail_prediction = session.run(fetches=[self.idx_head_prediction,\n",
        "                                                                            self.idx_tail_prediction],\n",
        "                                                                   feed_dict={self.eval_triple: eval_triple})\n",
        "            eval_result_queue.put((eval_triple, idx_head_prediction, idx_tail_prediction))\n",
        "            n_used_eval_triple += 1\n",
        "            print('[{:.3f}s] #evaluation triple: {}/{}'.format(timeit.default_timer() - start,\n",
        "                                                               n_used_eval_triple,\n",
        "                                                               self.kg.n_test_triple), end='\\r')\n",
        "        print()\n",
        "        for _ in range(self.n_rank_calculator):\n",
        "            eval_result_queue.put(None)\n",
        "        print('-----Joining all rank calculator-----')\n",
        "        eval_result_queue.join()\n",
        "        print('-----All rank calculation accomplished-----')\n",
        "        print('-----Obtaining evaluation results-----')\n",
        "        '''Raw'''\n",
        "        head_meanrank_raw = 0\n",
        "        head_hits10_raw = 0\n",
        "        tail_meanrank_raw = 0\n",
        "        tail_hits10_raw = 0\n",
        "        '''Filter'''\n",
        "        head_meanrank_filter = 0\n",
        "        head_hits10_filter = 0\n",
        "        tail_meanrank_filter = 0\n",
        "        tail_hits10_filter = 0\n",
        "        for _ in range(n_used_eval_triple):\n",
        "            head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = rank_result_queue.get()\n",
        "            head_meanrank_raw += head_rank_raw\n",
        "            if head_rank_raw < 10:\n",
        "                head_hits10_raw += 1\n",
        "            tail_meanrank_raw += tail_rank_raw\n",
        "            if tail_rank_raw < 10:\n",
        "                tail_hits10_raw += 1\n",
        "            head_meanrank_filter += head_rank_filter\n",
        "            if head_rank_filter < 10:\n",
        "                head_hits10_filter += 1\n",
        "            tail_meanrank_filter += tail_rank_filter\n",
        "            if tail_rank_filter < 10:\n",
        "                tail_hits10_filter += 1\n",
        "        print('-----Raw-----')\n",
        "        head_meanrank_raw /= n_used_eval_triple\n",
        "        head_hits10_raw /= n_used_eval_triple\n",
        "        tail_meanrank_raw /= n_used_eval_triple\n",
        "        tail_hits10_raw /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_raw, head_hits10_raw))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_raw, tail_hits10_raw))\n",
        "        print('------Average------')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_raw + tail_meanrank_raw) / 2,\n",
        "                                                         (head_hits10_raw + tail_hits10_raw) / 2))\n",
        "        print('-----Filter-----')\n",
        "        head_meanrank_filter /= n_used_eval_triple\n",
        "        head_hits10_filter /= n_used_eval_triple\n",
        "        tail_meanrank_filter /= n_used_eval_triple\n",
        "        tail_hits10_filter /= n_used_eval_triple\n",
        "        print('-----Head prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(head_meanrank_filter, head_hits10_filter))\n",
        "        print('-----Tail prediction-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format(tail_meanrank_filter, tail_hits10_filter))\n",
        "        print('-----Average-----')\n",
        "        print('MeanRank: {:.3f}, Hits@10: {:.3f}'.format((head_meanrank_filter + tail_meanrank_filter) / 2,\n",
        "                                                         (head_hits10_filter + tail_hits10_filter) / 2))\n",
        "        print('cost time: {:.3f}s'.format(timeit.default_timer() - start))\n",
        "        print('-----Finish evaluation-----')\n",
        "\n",
        "    def calculate_rank(self, in_queue, out_queue):\n",
        "        while True:\n",
        "            idx_predictions = in_queue.get()\n",
        "            if idx_predictions is None:\n",
        "                in_queue.task_done()\n",
        "                return\n",
        "            else:\n",
        "                eval_triple, idx_head_prediction, idx_tail_prediction = idx_predictions\n",
        "                head, tail, relation = eval_triple\n",
        "                head_rank_raw = 0\n",
        "                tail_rank_raw = 0\n",
        "                head_rank_filter = 0\n",
        "                tail_rank_filter = 0\n",
        "                for candidate in idx_head_prediction[::-1]:\n",
        "                    if candidate == head:\n",
        "                        break\n",
        "                    else:\n",
        "                        head_rank_raw += 1\n",
        "                        if (candidate, tail, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            head_rank_filter += 1\n",
        "                for candidate in idx_tail_prediction[::-1]:\n",
        "                    if candidate == tail:\n",
        "                        break\n",
        "                    else:\n",
        "                        tail_rank_raw += 1\n",
        "                        if (head, candidate, relation) in self.kg.golden_triple_pool:\n",
        "                            continue\n",
        "                        else:\n",
        "                            tail_rank_filter += 1\n",
        "                out_queue.put((head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter))\n",
        "                in_queue.task_done()\n",
        "\n",
        "    def check_norm(self, session):\n",
        "        print('-----Check norm-----')\n",
        "        entity_head_embedding = self.entity_head_embedding.eval(session=session)\n",
        "        entity_tail_embedding = self.entity_tail_embedding.eval(session=session)\n",
        "        relation_embedding = self.relation_embedding.eval(session=session)\n",
        "        head_entity_norm = np.linalg.norm(entity_head_embedding, ord=2, axis=1)\n",
        "        tail_entity_norm = np.linalg.norm(entity_tail_embedding, ord=2, axis=1)\n",
        "        relation_norm = np.linalg.norm(relation_embedding, ord=2, axis=1)\n",
        "        #print(entity_norm)\n",
        "        #print(entity_embedding)\n",
        "        # entity_embedding_list = []\n",
        "        # entity_embedding_norm_list = []\n",
        "        # entity_embedding_list.append(entity_embedding)\n",
        "        # entity_embedding_norm_list.append(entity_norm)\n",
        "\n",
        "        with open('imdb30_c1_head_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_head_embedding)\n",
        "\n",
        "        with open('imdb30_c1_tail_entity_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(entity_tail_embedding)\n",
        "\n",
        "        with open('imdb30_c1_relation_embedding_100.csv','w') as file :\n",
        "          write = csv.writer(file)\n",
        "          write.writerows(relation_embedding)\n",
        "\n",
        "        # with open('List_norm1.csv','w') as file :\n",
        "        #   write = csv.writer(file)\n",
        "        #   write.writerows(entity_norm)\n",
        "\n",
        "        #return entity_embedding_list #, entity_embedding_norm_list\n",
        "        \n",
        "        #print('entity norm: {} relation norm: {}'.format(entity_norm, relation_norm))\n",
        "\n",
        "    def save(self, session, data_dir):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(session, data_dir, global_step=self.global_step.eval())\n",
        "\n",
        "    #     # tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
        "    # def save(self, session, data_dir):\n",
        "    #   SAVER_DIR = \"model\"\n",
        "    #   saver = tf.compat.v1.train.Saver()\n",
        "    #   checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
        "    #   ckpt = tf.compat.v1.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description='TransE')\n",
        "    # parser.add_argument('--data_dir', type=str, default='../data/FB15k/')\n",
        "    # parser.add_argument('--embedding_dim', type=int, default=200)\n",
        "    # parser.add_argument('--margin_value', type=float, default=1.0)\n",
        "    # parser.add_argument('--score_func', type=str, default='L1')\n",
        "    # parser.add_argument('--batch_size', type=int, default=4800)\n",
        "    # parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "    # parser.add_argument('--n_generator', type=int, default=24)\n",
        "    # parser.add_argument('--n_rank_calculator', type=int, default=24)\n",
        "    # parser.add_argument('--ckpt_dir', type=str, default='../ckpt/')\n",
        "    # parser.add_argument('--summary_dir', type=str, default='../summary/')\n",
        "    # parser.add_argument('--max_epoch', type=int, default=500)\n",
        "    # parser.add_argument('--eval_freq', type=int, default=10)\n",
        "    #args = parser.parse_args()\n",
        "    #print(args)\n",
        "    data_dir = (\"/content/\")\n",
        "    embedding_dim = 100\n",
        "    margin_value = 1.0\n",
        "    score_func = \"L1\"\n",
        "    batch_size =4800\n",
        "    learning_rate = 0.01\n",
        "    n_generator =24\n",
        "    n_rank_calculator = 24\n",
        "    ckpt_dir = '../ckpt/'\n",
        "    summary_dir = '../summary/'\n",
        "    max_epoch = 1000\n",
        "    eval_freq = 10\n",
        "\n",
        "     \n",
        "    kg = KnowledgeGraph(data_dir=data_dir)\n",
        "    kge_model = TransE(kg=kg, embedding_dim=embedding_dim, margin_value=margin_value,\n",
        "                       score_func=score_func, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       n_generator=n_generator, n_rank_calculator=n_rank_calculator)\n",
        "    \n",
        "    gpu_config = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.compat.v1.ConfigProto(gpu_options=gpu_config)\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        print('-----Initializing tf graph-----')\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('-----Initialization accomplished-----')\n",
        "        kge_model.check_norm(session=sess)\n",
        "        summary_writer = tf.compat.v1.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
        "        \n",
        "        for epoch in range(max_epoch):\n",
        "            print('=' * 30 + '[EPOCH {}]'.format(epoch) + '=' * 30)\n",
        "            kge_model.launch_training(session=sess, summary_writer=summary_writer)\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                kge_model.launch_evaluation(session=sess)\n",
        "        kge_model.save( session = sess, data_dir = data_dir)  \n",
        "        print(\"---------save-----------\")    \n",
        "        #kge_model.check_norm(sess)\n",
        "        #return entity_embedding_list, entity_embedding_norm_list"
      ],
      "metadata": {
        "id": "7otBLXbJclnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "mBXeWitpc8oo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}